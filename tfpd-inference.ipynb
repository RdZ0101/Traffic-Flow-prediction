{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807be91d-4369-4fb8-9b85-e80ebc6d0b88",
   "metadata": {},
   "source": [
    "# TFPD-Inference Notebook\n",
    "Note: distances between intersections are calculated as the crow flies, aka straight line distance, between their GPS coordinates. Thus, they do not account for curving roads and such.\n",
    "\n",
    "What's been done? (we should update this as more gets done):\n",
    "- We can get the straight line distance between any two given intersections\n",
    "- We can get the estimated vehicle flow for a given intersection given input of (lag) previous traffic measurements.\n",
    "- We can estimate vehicle speed given vflow (as per assumption (ii). another assumption: road is always under capacity, never over)\n",
    "- We can find the shortest path between two intersections using Djikstra's algorithm. This is currently only based on distance. We only need to add travel time penalties to the 'dist' variable.\n",
    "\n",
    "What's left:\n",
    "- How do we obtain (lag) previous traffic measurements? Are we supposed to use test / dummy data for this?\n",
    "- 'Dist' should be converted to a 'travel time'-esque variable. This can be calculated like so after we figure out the point above:\n",
    "    - x = distance in kilometers\n",
    "    - y = kmh predicted at intersection. caps at 60kmh as per assumption (i)\n",
    "    - (x / y) * 60 * 60 + 30\n",
    "    - multiply by 60 twice to convert hours to seconds\n",
    "    - add 30 secs for assumption (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a78a22-32d7-44bb-823d-b6abcd6f560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 00:30:20.656299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-24 00:30:20.733663: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-24 00:30:20.754150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 00:30:20.883331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 00:30:21.976671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "from geopy.distance import geodesic\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.src.legacy.saving import legacy_h5_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dace555-0108-47d5-8b8c-469236a4f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to reconstruct scaler... we should save scalers :(\n",
    "def process_data(train, test, lags):\n",
    "    \"\"\"Process data\n",
    "    Reshape and split train\\test data.\n",
    "\n",
    "    # Arguments\n",
    "        train: String, name of .csv train file.\n",
    "        test: String, name of .csv test file.\n",
    "        lags: integer, time lag.\n",
    "    # Returns\n",
    "        X_train: ndarray.\n",
    "        y_train: ndarray.\n",
    "        X_test: ndarray.\n",
    "        y_test: ndarray.\n",
    "        scaler: StandardScaler.\n",
    "    \"\"\"\n",
    "    attr = 'VFlow'\n",
    "    df1 = pd.read_csv(train, encoding='utf-8').fillna(0)\n",
    "    df2 = pd.read_csv(test, encoding='utf-8').fillna(0)\n",
    "\n",
    "    # scaler = StandardScaler().fit(df1[attr].values)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(df1[attr].values.reshape(-1, 1))\n",
    "    flow1 = scaler.transform(df1[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    flow2 = scaler.transform(df2[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    train, test = [], []\n",
    "    for i in range(lags, len(flow1)):\n",
    "        train.append(flow1[i - lags: i + 1])\n",
    "    for i in range(lags, len(flow2)):\n",
    "        test.append(flow2[i - lags: i + 1])\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    np.random.shuffle(train)\n",
    "\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    X_test = test[:, :-1]\n",
    "    y_test = test[:, -1]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af43c573-7106-4bb4-8aa3-7ff4285dc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: x (float) = vflow per hour\n",
    "# output: float = speed limit or speed calculated from flow, whichever lower\n",
    "def get_speed_from_flow_per_hr(x):   \n",
    "    # v = inflection speed, aka average kmh when road is at vflow q\n",
    "    v = 32\n",
    "    # q = inflection point, where speed converges for given vflow when road is over or under capacity\n",
    "    q = 1500\n",
    "\n",
    "    # note: for this assignment, i believe we are always assuming\n",
    "    # roads are under capacity. thus this is just a high number\n",
    "    road_capacity = 9999\n",
    "    \n",
    "    a = -(q/(v*v))\n",
    "    b = -2*v*a\n",
    "\n",
    "    # use this when road is over capacity (x >= q)\n",
    "    # this means speed decreases given lower vflow\n",
    "    if x >= road_capacity:\n",
    "        speed_from_flow = (-b + math.sqrt(b*b + 4*a*x)) / (2*a)\n",
    "    # use this when road is under capacity (x < q)\n",
    "    # this means speed increases given lower vflow\n",
    "    else:\n",
    "        speed_from_flow = (-b - math.sqrt(b*b + 4*a*x)) / (2*a)\n",
    "\n",
    "    # assumed speed limit of 60kmh\n",
    "    speed_limit = 60\n",
    "    return min(speed_limit, speed_from_flow)\n",
    "\n",
    "# returns: predicted value, model to cache, scaler to cache\n",
    "def get_est_vflow_for_intersection(scats, lag_flow, cache_model, cache_scaler):\n",
    "    if cache_model is False:\n",
    "        model_variant = 'gru'\n",
    "        model = load_model(f\"model/{model_variant}/{scats}/{model_variant}_{scats}.h5\", custom_objects={'mse': 'mse'})\n",
    "    else:\n",
    "        model = cache_model\n",
    "\n",
    "    if cache_scaler is False:\n",
    "        lag = 12\n",
    "        \n",
    "        train_folder = 'intersection/train/'\n",
    "        test_folder = 'intersection/test/'\n",
    "    \n",
    "        train_file = os.path.join(train_folder, f'train_{scats}.csv')\n",
    "        test_file = os.path.join(test_folder, f\"test_{scats}.csv\")\n",
    "    \n",
    "        _, _, X_test, y_test, scaler = process_data(train_file, test_file, lag)\n",
    "    else:\n",
    "        scaler = cache_scaler\n",
    "\n",
    "    lag_flow = scaler.transform(lag_flow)\n",
    "    lag_flow_reshaped = np.array([lag_flow])\n",
    "    \n",
    "    predicted = model.predict(lag_flow_reshaped, verbose=0)\n",
    "    predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    return predicted, model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e613cc2f-e486-458c-a4a1-ece9ac3c89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put lat,long of intersection 1 and 2 respectively here\n",
    "def calculate_distance_for_coords(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49347ce4-9cb0-4b7c-8627-cfb1755024e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# parse d/MM/YYYY format to DateTime\n",
    "def parse_date(date_string):\n",
    "    split = date_string.split('/')\n",
    "\n",
    "    if len(split) != 3:\n",
    "        print(f\"invalid date format: {date_string}\")\n",
    "\n",
    "    day = int(split[0])\n",
    "    month = int(split[1])\n",
    "    year = int(split[2])\n",
    "\n",
    "    return datetime.datetime(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1068c4-d3b9-40aa-85ce-23f9ae882f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns string such as 'V01' or 'V93' given n=1 or n=93\n",
    "def append_v(n):\n",
    "    n = int(n)\n",
    "    if n < 10:\n",
    "        column_name = \"V0\" + str(n)\n",
    "    else:\n",
    "        column_name = \"V\" + str(n)\n",
    "        \n",
    "    return column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9715686b-b6f1-464a-ab12-4f418b11935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df - pandas DataFrame: scats dataframe file\n",
    "# scats - int: scats number of intersection\n",
    "# lag - int: number of previous time intervals to append\n",
    "# weekday - int: day of week from 0-6 (0=Mon, 6=Sun)\n",
    "# time - int: time of day in minutes (0 = 12:00am, 60 = 1:00am, 720 = 12:00pm, 1439 = 11:59pm, etc.)\n",
    "def get_average_lag(df, scats, lag, weekday, time):\n",
    "    V_INTERVAL = 15 # interval of data in minutes\n",
    "    V_MAX = int(1440 / V_INTERVAL) - 1 # 1440 = minutes a day, subtract 1 for zero indexing\n",
    "    \n",
    "    intersection_df = df[df['SCATS Number'] == scats]\n",
    "\n",
    "    while time >= 1440:\n",
    "        time -= 1440\n",
    "        weekday += 1\n",
    "        if weekday > 6:\n",
    "            weekday = 0\n",
    "    \n",
    "    v_base = int(math.floor(time / V_INTERVAL))\n",
    "\n",
    "    lags = []\n",
    "    \n",
    "    for i in range(0, lag):\n",
    "        cur_weekday = weekday\n",
    "        v = v_base - (lag - i)        \n",
    "        if v < 0:\n",
    "            v = V_MAX + v + 1 # add 1 as -1 should wrap around to V_MAX, not V_MAX-1\n",
    "            cur_weekday = weekday - 1 # wrap around to previous weekday\n",
    "            if cur_weekday < 0:\n",
    "                cur_weekday = 6 # wrap back to Sunday if we were on Monday\n",
    "\n",
    "        time_lags_total = 0\n",
    "        time_lags_count = 0\n",
    "        for index, row in intersection_df.iterrows():\n",
    "            date_of = parse_date(row['Date'])\n",
    "            if date_of.weekday() is not cur_weekday:\n",
    "                continue\n",
    "\n",
    "            flow_for_day = row[append_v(v)]\n",
    "            time_lags_total += flow_for_day\n",
    "            time_lags_count += 1\n",
    "\n",
    "        if time_lags_count <= 0:\n",
    "            print(\"invalid data? didn't find any data for lag period\")\n",
    "            continue\n",
    "\n",
    "        lags.append([time_lags_total / time_lags_count])\n",
    "\n",
    "    return lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "812ab190-9b5b-4a05-be34-69234293cfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[145.1875],\n",
       " [115.4375],\n",
       " [95.625],\n",
       " [95.8125],\n",
       " [90.875],\n",
       " [87.3125],\n",
       " [84.9375],\n",
       " [85.6875],\n",
       " [74.9375],\n",
       " [70.375],\n",
       " [61.1875],\n",
       " [55.6875]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of input to get_average_lag function\n",
    "# scats_data = pd.read_csv(scats_data_file)\n",
    "# 970 = scats intersection\n",
    "# 12 = lag number (12 intervals)\n",
    "# 5 = weekday (saturday)\n",
    "# 15 = time of 12:15AM\n",
    "get_average_lag(scats_data, 970, 12, 5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "407cf715-b922-46c4-bd7d-7d712a3a0b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.58211681840301"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of input to the get_speed_from_flow_per_hr function\n",
    "get_speed_from_flow_per_hr(753)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83106144-8915-410d-adce-3e0ece31f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "[16.961971]\n"
     ]
    }
   ],
   "source": [
    "# Example of input to the get_est_vflow_for_intersection function\n",
    "lag_flow = np.array([[86], [83], [52], [58], [59], [44], [31], [37], [30], [24], [16], [24]])\n",
    "predicted = get_est_vflow_for_intersection(970, lag_flow)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566a4c03-54bc-46e0-aa9c-91489bf89fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, lat, long, neighbors):\n",
    "        self.cost = 99999 # cost to reach this node from start node\n",
    "        self.lat = lat # latitude of node (for dist calculations)\n",
    "        self.long = long # longitude of node (for dist calculations)\n",
    "        self.prev = -1 # scats number of previous node\n",
    "        self.neighbors = neighbors # scats numbers of neighbors in list form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e9cc77e3-9145-4f0c-b491-f9ce5226f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_df = scats_data[scats_data['SCATS Number'] == 970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53886bce-629e-409b-9029-ddd65e22157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729690247.595867     706 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-24 00:30:47.723582: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2cdc4171c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2cdc4171c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2cdc34c790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2cdc34c790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost of path: 237.07223607269046\n",
      "[4264, 4040, 3812]\n"
     ]
    }
   ],
   "source": [
    "scats_neighbors_file = 'data/scats_neighbors.csv'\n",
    "\n",
    "scats_data_file = 'scats-10-2006.csv'\n",
    "scats_data = pd.read_csv(scats_data_file)\n",
    "\n",
    "# using djikstra's\n",
    "# load intersections\n",
    "scats_sites = pd.read_csv(scats_neighbors_file)\n",
    "\n",
    "neighbor_cols = ['North Neighbor', 'Northeast Neighbor', 'East Neighbor', \n",
    "'Southeast Neighbor', 'South Neighbor' ,'Southwest Neighbor',\n",
    "'West Neighbor', 'Northwest Neighbor']\n",
    "\n",
    "for neighbor_dir in neighbor_cols:\n",
    "    # scats sites can't be negative, so set NaNs to -1\n",
    "    scats_sites[neighbor_dir].fillna(-1, inplace=True)\n",
    "    \n",
    "start_scats = 4264\n",
    "target_scats = 3812\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "#base_sec_time = (time - time.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()\n",
    "base_sec_time = 1080\n",
    "weekday = time.weekday()\n",
    "\n",
    "start_intersection = scats_sites[scats_sites['SCAT number'] == start_scats]\n",
    "\n",
    "def construct_node(pd_row):\n",
    "    neighbors = []\n",
    "    \n",
    "    for neighbor_dir in neighbor_cols:\n",
    "        neighbor = pd_row[neighbor_dir].item()\n",
    "        if neighbor > 0:\n",
    "            neighbors.append(int(neighbor))\n",
    "\n",
    "    return Node(pd_row['NB_Latitude'], pd_row['NB_Longitude'], neighbors)\n",
    "\n",
    "cache = {}\n",
    "\n",
    "graph = {}\n",
    "queue = []\n",
    "\n",
    "# construct graph of all nodes\n",
    "for index, row in scats_sites.iterrows():\n",
    "    scats_num = int(row['SCAT number'])\n",
    "    graph[scats_num] = construct_node(scats_sites.iloc[index])\n",
    "    queue.append(scats_num)\n",
    "\n",
    "# set start node cost to 0\n",
    "graph[start_scats].cost = 0\n",
    "path_cost = -1\n",
    "\n",
    "while queue:\n",
    "    # get vertex in queue with minimum cost value\n",
    "    min_cost, min_scats = 99999, -1\n",
    "    for i in queue:\n",
    "        node = graph[i]\n",
    "        if node.cost < min_cost:\n",
    "            min_cost = node.cost\n",
    "            min_scats = i\n",
    "            \n",
    "    u = min_scats\n",
    "    u_node = graph[u]\n",
    "    \n",
    "    if u == target_scats:\n",
    "        path_cost = u_node.cost\n",
    "        break\n",
    "    \n",
    "    queue.remove(min_scats)\n",
    "\n",
    "    for v in u_node.neighbors:\n",
    "        v_node = graph[v]\n",
    "\n",
    "        # distance = cost heuristic (travel time between the two intersections)\n",
    "        # divide by 1000 as output is in meters\n",
    "        distance = calculate_distance_for_coords(u_node.lat, u_node.long, v_node.lat, v_node.long) / 1000\n",
    "\n",
    "        cost_in_minutes = int(u_node.cost / 60)\n",
    "        cur_time = int(base_sec_time + cost_in_minutes)\n",
    "        lags_data = get_average_lag(scats_data, v, 12, weekday, cur_time)\n",
    "        lags_data = np.array(lags_data)\n",
    "\n",
    "        if v in cache:\n",
    "            cached = cache[v]\n",
    "            model_output, _, _ = get_est_vflow_for_intersection(v, lags_data, cached[0], cached[1])\n",
    "        else:\n",
    "            model_output, model_to_cache, scaler_to_cache = get_est_vflow_for_intersection(v, lags_data, False, False)\n",
    "            cache[v] = (model_to_cache, scaler_to_cache)\n",
    "\n",
    "        vflow = model_output[0]\n",
    "\n",
    "        # multiply by 4 to get flow per hour from flow per 15 mins\n",
    "        y = get_speed_from_flow_per_hr(vflow * 4)\n",
    "\n",
    "        # cost = travel time to this node in seconds from start\n",
    "        # convert from hours to seconds (y is in kmh) then\n",
    "        # add 30 seconds for assumption iii\n",
    "        cost = (distance / y) * 60 * 60 + 30\n",
    "        \n",
    "        alt = u_node.cost + cost\n",
    "        if alt < v_node.cost:\n",
    "            v_node.cost = alt\n",
    "            v_node.prev = u\n",
    "\n",
    "path = [u]\n",
    "prevy = u_node.prev\n",
    "while prevy > 0:\n",
    "    path.insert(0, prevy)\n",
    "    prevy = graph[prevy].prev\n",
    "\n",
    "print(f'cost of path: {path_cost}')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf67209-a09e-4a26-b856-928dbd85e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
