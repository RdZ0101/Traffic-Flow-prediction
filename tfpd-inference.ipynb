{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807be91d-4369-4fb8-9b85-e80ebc6d0b88",
   "metadata": {},
   "source": [
    "# TFPD-Inference Notebook\n",
    "Note: distances between intersections are calculated as the crow flies, aka straight line distance, between their GPS coordinates. Thus, they do not account for curving roads and such.\n",
    "\n",
    "What's been done? (we should update this as more gets done):\n",
    "- We can get the straight line distance between any two given intersections\n",
    "- We can get the estimated vehicle flow for a given intersection given input of (lag) previous traffic measurements.\n",
    "- We can estimate vehicle speed given vflow (as per assumption (ii). another assumption: road is always under capacity, never over)\n",
    "- We can find the shortest path between two intersections using Djikstra's algorithm. This is currently only based on distance. We only need to add travel time penalties to the 'dist' variable.\n",
    "\n",
    "What's left:\n",
    "- How do we obtain (lag) previous traffic measurements? Are we supposed to use test / dummy data for this?\n",
    "- 'Dist' should be converted to a 'travel time'-esque variable. This can be calculated like so after we figure out the point above:\n",
    "    - x = distance in kilometers\n",
    "    - y = kmh predicted at intersection. caps at 60kmh as per assumption (i)\n",
    "    - (x / y) * 60 * 60 + 30\n",
    "    - multiply by 60 twice to convert hours to seconds\n",
    "    - add 30 secs for assumption (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a78a22-32d7-44bb-823d-b6abcd6f560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 02:07:57.382381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 02:07:57.393988: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 02:07:57.397091: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-17 02:07:57.405386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 02:07:57.919388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "from geopy.distance import geodesic\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.src.legacy.saving import legacy_h5_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dace555-0108-47d5-8b8c-469236a4f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to reconstruct scaler... we should save scalers :(\n",
    "def process_data(train, test, lags):\n",
    "    \"\"\"Process data\n",
    "    Reshape and split train\\test data.\n",
    "\n",
    "    # Arguments\n",
    "        train: String, name of .csv train file.\n",
    "        test: String, name of .csv test file.\n",
    "        lags: integer, time lag.\n",
    "    # Returns\n",
    "        X_train: ndarray.\n",
    "        y_train: ndarray.\n",
    "        X_test: ndarray.\n",
    "        y_test: ndarray.\n",
    "        scaler: StandardScaler.\n",
    "    \"\"\"\n",
    "    attr = 'VFlow'\n",
    "    df1 = pd.read_csv(train, encoding='utf-8').fillna(0)\n",
    "    df2 = pd.read_csv(test, encoding='utf-8').fillna(0)\n",
    "\n",
    "    # scaler = StandardScaler().fit(df1[attr].values)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(df1[attr].values.reshape(-1, 1))\n",
    "    flow1 = scaler.transform(df1[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    flow2 = scaler.transform(df2[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    train, test = [], []\n",
    "    for i in range(lags, len(flow1)):\n",
    "        train.append(flow1[i - lags: i + 1])\n",
    "    for i in range(lags, len(flow2)):\n",
    "        test.append(flow2[i - lags: i + 1])\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    np.random.shuffle(train)\n",
    "\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    X_test = test[:, :-1]\n",
    "    y_test = test[:, -1]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af43c573-7106-4bb4-8aa3-7ff4285dc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed_from_flow_per_hr(x):   \n",
    "    # v = inflection speed, aka average kmh when road is at vflow q\n",
    "    v = 32\n",
    "    # q = inflection point, where speed converges for given vflow when road is over or under capacity\n",
    "    q = 1500\n",
    "\n",
    "    # note: for this assignment, i believe we are always assuming\n",
    "    # roads are under capacity. thus this is just a high number\n",
    "    road_capacity = 9999\n",
    "    \n",
    "    a = -(q/(v*v))\n",
    "    b = -2*v*a\n",
    "\n",
    "    # use this when road is over capacity (x >= q)\n",
    "    # this means speed decreases given lower vflow\n",
    "    if x >= road_capacity:\n",
    "        speed_from_flow = (-b + math.sqrt(b*b + 4*a*x)) / (2*a)\n",
    "    # use this when road is under capacity (x < q)\n",
    "    # this means speed increases given lower vflow\n",
    "    else:\n",
    "        speed_from_flow = (-b - math.sqrt(b*b + 4*a*x)) / (2*a)\n",
    "\n",
    "    # assumed speed limit of 60kmh\n",
    "    speed_limit = 60\n",
    "    return min(speed_limit, speed_from_flow)\n",
    "\n",
    "def get_est_vflow_for_intersection(scats, lag_flow):\n",
    "    model_variant = 'gru'\n",
    "    model = load_model(f\"model/{model_variant}/{scats}/{model_variant}_{scats}.h5\", custom_objects={'mse': 'mse'})\n",
    "\n",
    "    lag = 12\n",
    "    \n",
    "    train_folder = 'intersection/train/'\n",
    "    test_folder = 'intersection/test/'\n",
    "\n",
    "    train_file = os.path.join(train_folder, f'train_{scats}.csv')\n",
    "    test_file = os.path.join(test_folder, f\"test_{scats}.csv\")\n",
    "\n",
    "    _, _, X_test, y_test, scaler = process_data(train_file, test_file, lag)\n",
    "\n",
    "    lag_flow = scaler.transform(lag_flow)\n",
    "    lag_flow_reshaped = np.array([lag_flow])\n",
    "    \n",
    "    predicted = model.predict(lag_flow_reshaped)\n",
    "    predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e613cc2f-e486-458c-a4a1-ece9ac3c89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put lat,long of intersection 1 and 2 respectively here\n",
    "def calculate_distance_for_coords(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407cf715-b922-46c4-bd7d-7d712a3a0b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.58211681840301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of input to the get_speed_from_flow_per_hr function\n",
    "get_speed_from_flow_per_hr(753)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83106144-8915-410d-adce-3e0ece31f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729091278.647053   88258 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-17 02:07:58.664683: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "[16.961971]\n"
     ]
    }
   ],
   "source": [
    "# Example of input to the get_est_vflow_for_intersection function\n",
    "lag_flow = np.array([[86], [83], [52], [58], [59], [44], [31], [37], [30], [24], [16], [24]])\n",
    "predicted = get_est_vflow_for_intersection(970, lag_flow)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566a4c03-54bc-46e0-aa9c-91489bf89fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, lat, long, neighbors):\n",
    "        self.dist = 99999 # dist from start node\n",
    "        self.lat = lat # latitude of node (for dist calculations)\n",
    "        self.long = long # longitude of node (for dist calculations)\n",
    "        self.prev = -1 # scats number of previous node\n",
    "        self.neighbors = neighbors # scats numbers of neighbors in list form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53886bce-629e-409b-9029-ddd65e22157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[970, 3685, 2000, 3682, 3126, 3127, 4063, 4057, 3180]\n"
     ]
    }
   ],
   "source": [
    "scats_neighbors_file = 'data/scats_neighbors.csv'\n",
    "scats_data_file = 'scats-10-2006.csv'\n",
    "\n",
    "# using djikstra's\n",
    "# load intersections\n",
    "scats_sites = pd.read_csv(scats_neighbors_file)\n",
    "\n",
    "neighbor_cols = ['North Neighbor', 'East Neighbor', 'South Neighbor', 'West Neighbor']\n",
    "\n",
    "for neighbor_dir in neighbor_cols:\n",
    "    # scats sites can't be negative, so set NaNs to -1\n",
    "    scats_sites[neighbor_dir].fillna(-1, inplace=True)\n",
    "    # convert to int so we can look up sites easier\n",
    "    scats_sites[neighbor_dir] = scats_sites[neighbor_dir].astype(int)\n",
    "\n",
    "start_scats = 970\n",
    "target_scats = 3180\n",
    "\n",
    "start_intersection = scats_sites[scats_sites['SCAT number'] == start_scats]\n",
    "\n",
    "def construct_node(pd_row):\n",
    "    neighbors = []\n",
    "    \n",
    "    for neighbor_dir in neighbor_cols:\n",
    "        neighbor = pd_row[neighbor_dir].item()\n",
    "        if neighbor > 0:\n",
    "            neighbors.append(neighbor)\n",
    "\n",
    "    return Node(pd_row['NB_Latitude'], pd_row['NB_Longitude'], neighbors)\n",
    "\n",
    "graph = {}\n",
    "queue = []\n",
    "\n",
    "# construct graph of all nodes\n",
    "for index, row in scats_sites.iterrows():\n",
    "    scats_num = row['SCAT number']\n",
    "    graph[scats_num] = construct_node(scats_sites.iloc[index])\n",
    "    queue.append(scats_num)\n",
    "\n",
    "# set start node distance to 0 (\n",
    "graph[start_scats].dist = 0\n",
    "\n",
    "while queue:\n",
    "    # get vertex in queue with minimum dist value\n",
    "    min_dist, min_scats = 99999, -1\n",
    "    for i in queue:\n",
    "        node = graph[i]\n",
    "        if node.dist < min_dist:\n",
    "            min_dist = node.dist\n",
    "            min_scats = i\n",
    "            \n",
    "    u = min_scats\n",
    "    u_node = graph[u]\n",
    "    \n",
    "    if u == target_scats:\n",
    "        break\n",
    "    \n",
    "    queue.remove(min_scats)\n",
    "\n",
    "    for v in u_node.neighbors:\n",
    "        v_node = graph[v]\n",
    "\n",
    "        # distance = cost heuristic (travel time between the two intersections)\n",
    "        distance = calculate_distance_for_coords(u_node.lat, u_node.long, v_node.lat, v_node.long)\n",
    "        \n",
    "        alt = u_node.dist + distance\n",
    "        if alt < v_node.dist:\n",
    "            v_node.dist = alt\n",
    "            v_node.prev = u\n",
    "\n",
    "path = [u]\n",
    "prevy = u_node.prev\n",
    "while prevy > 0:\n",
    "    path.insert(0, prevy)\n",
    "    prevy = graph[prevy].prev\n",
    "\n",
    "print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
